{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka2Zb6YD2DLR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.pipeline import Pipeline as imbpipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "gnyepB8y2sv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/Applied research project/Data_X/Final-V14-news_updated.xlsx\"\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "cWSgXCmo2vmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Feature engineering\n",
        "\n",
        "# Convert 'ADJ_FOUNDED_ON' to datetime and extract the founding year\n",
        "df['founded_year'] = pd.to_datetime(df['ADJ_FOUNDED_ON'], errors='coerce').dt.year\n",
        "\n",
        "def extract_exited_year(value):\n",
        "    # If the startup still not exited\n",
        "    if isinstance(value, str) and value.strip().lower() == \"not exited yet\":\n",
        "        # Use 0\n",
        "        return 0\n",
        "    else:\n",
        "        # Try to convert the value to a datetime and extract the year\n",
        "        try:\n",
        "            return pd.to_datetime(value, errors='coerce').year\n",
        "        except Exception as e:\n",
        "            return 0\n",
        "\n",
        "# Apply the function to create a new 'closed_year' column\n",
        "df['exit_year'] = df['ADJ_EXITED_ON'].apply(extract_exited_year)\n",
        "\n",
        "# Convert datetime columns to numerical before setting X and y\n",
        "datetime_cols = df.select_dtypes(include=['datetime64']).columns\n",
        "for col in datetime_cols:\n",
        "    df[col] = df[col].apply(lambda x: x.toordinal() if pd.notnull(x) else x)\n",
        "\n",
        "# Apply log transformation to reduce skew on money columns.\n",
        "df['Average_Money_Raised_log'] = np.log(df['Average_Money_Raised'] + 1)\n",
        "df['Total_Money_Raised_log'] = np.log(df['Toatl_Money_Raised'] + 1)\n",
        "df['Money_Raised_Ratio'] = df['Average_Money_Raised_log'] / (df['Total_Money_Raised_log'] + 1e-8)\n",
        "\n",
        "## Check for missing values in the entire DataFrame\n",
        "total_nans = df.isnull().sum().sum()\n",
        "print(\"Total NaNs in DataFrame:\", total_nans)\n",
        "\n",
        "# List all columns that contain NaN values\n",
        "nan_columns = df.columns[df.isnull().any()].tolist()\n",
        "print(\"Columns with NaN values:\", nan_columns)\n",
        "\n",
        "# If you suspect empty strings may be an issue, replace them with np.nan and check again:\n",
        "df_cleaned = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "nan_columns_after_replace = df_cleaned.columns[df_cleaned.isnull().any()].tolist()\n",
        "print(\"Columns with NaN values after replacing empty strings:\", nan_columns_after_replace)\n",
        "\n",
        "# Check for infinite values in numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number])\n",
        "inf_counts = (numeric_cols == np.inf).sum() + (numeric_cols == -np.inf).sum()\n",
        "inf_columns = inf_counts[inf_counts > 0].index.tolist()\n",
        "print(\"Columns with infinite values:\", inf_columns)\n",
        "\n"
      ],
      "metadata": {
        "id": "Cg9t3Fxu20Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle NaN in investment\n",
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "G7CET5cj25rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New feature calculate the currunt year-founding year and divide the total news to get average news per year\n",
        "# Get the current year furst\n",
        "current_year = pd.Timestamp.now().year\n",
        "\n",
        "# Calculate the 'Yearly_Exposure' column\n",
        "df['Yearly_Exposure'] = df['Total_News'] / (current_year - df['founded_year'] + 1)"
      ],
      "metadata": {
        "id": "fKyupGBnLjQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['High_Positive_News_Rule2'] = (\n",
        "    (df['SUM_Sentiment_Category_Positive'] + df['SUM_Sentiment_Category_Very Positive']) >\n",
        "     (df['SUM_Sentiment_Category_Negative'] + df['SUM_Sentiment_Category_Very Negative'])\n",
        ").astype(int)\n",
        "\n",
        "# Class distribution for the new target\n",
        "new_dist = df['High_Positive_News_Rule2'].value_counts().rename_axis('Class').reset_index(name='Count')\n",
        "new_dist['Proportion'] = (new_dist['Count'] / len(df)).round(3)\n",
        "new_dist"
      ],
      "metadata": {
        "id": "7jRiQnL728Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['Total_News'] != 0]\n",
        "new_dist = df['High_Positive_News_Rule2'].value_counts().rename_axis('Class').reset_index(name='Count')\n",
        "new_dist['Proportion'] = (new_dist['Count'] / len(df)).round(3)\n",
        "new_dist"
      ],
      "metadata": {
        "id": "x5tpKFxCfPg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the rows where \"total news\" column has a 0 value\n",
        "df = df[df['Total_News'] != 0]\n",
        "# Define feature matrix X and target y.\n",
        "X = df.drop(['High_Positive_News_Rule2','Average_Money_Raised', 'Toatl_Money_Raised', 'Total_Money_Raised_log','Average_Money_Raised_log',\n",
        "             'ADJ_FOUNDED_ON','ADJ_EXITED_ON', 'ADJ_CLOSED_ON','COMPANY_ID','Total_News',\n",
        "             'Acquihire', 'Acquisition', 'Leveraged Buyout', 'Merger', 'No Acquisition Type', 'Acquihire_dummy', 'Acquisition_dummy', 'Leveraged Buyout_dummy', 'Merger_dummy', 'NoAcquisitionType_dummy'], axis=1)\n",
        "# sentiment_cols = [col for col in X.columns if 'Sentiment_Category' in col or 'Total_News' in col]\n",
        "sentiment_cols = [col for col in X.columns if 'Sentiment_Category' in col]\n",
        "\n",
        "X = X.drop(sentiment_cols, axis=1)\n",
        "y = df['High_Positive_News_Rule2']"
      ],
      "metadata": {
        "id": "gssuVVF82_Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all features in X\n",
        "for col in X.columns:\n",
        "    print(col)"
      ],
      "metadata": {
        "id": "GiQ32QAj_fgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pprint\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.base import clone\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# Helper function: compute metrics for given true labels, predictions, and probability estimates.\n",
        "def get_metrics(y_true, y_pred, y_proba):\n",
        "    metrics = {}\n",
        "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "    metrics['precision'] = precision_score(y_true, y_pred, zero_division=0)\n",
        "    metrics['recall'] = recall_score(y_true, y_pred, zero_division=0)\n",
        "    metrics['f1'] = f1_score(y_true, y_pred, zero_division=0)\n",
        "    if y_proba is not None:\n",
        "        metrics['auc'] = roc_auc_score(y_true, y_proba)\n",
        "    else:\n",
        "        metrics['auc'] = np.nan\n",
        "    return metrics\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# Core function: evaluate a model using cross validation with a given balancing strategy.\n",
        "def evaluate_model(model, X, y, balancing, cv_splits=5, random_state=42):\n",
        "    \"\"\"\n",
        "    balancing: one of 'none', 'smote', or 'class_weight'\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
        "    train_metrics_list = []\n",
        "    test_metrics_list = []\n",
        "\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        # Use .iloc if X and y are pandas objects to select rows by integer location.\n",
        "        try:\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "        except AttributeError:\n",
        "            # If X and y are numpy arrays.\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        sample_weights = None  # For class weighting (if needed)\n",
        "\n",
        "        # Apply SMOTE on the training data only.\n",
        "        if balancing == 'smote':\n",
        "            sm = SMOTE(random_state=random_state)\n",
        "            X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "        # For class_weight balancing:\n",
        "        if balancing == 'class_weight':\n",
        "            # For MLPClassifier, sample_weight is not supported.\n",
        "            if isinstance(model, MLPClassifier):\n",
        "                sample_weights = None\n",
        "            else:\n",
        "                classes = np.unique(y_train)\n",
        "                weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "                class_weight_dict = dict(zip(classes, weights))\n",
        "                sample_weights = np.array([class_weight_dict[label] for label in y_train])\n",
        "\n",
        "        # Fit the model with or without sample weights.\n",
        "        if sample_weights is not None:\n",
        "            model.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "        # In-sample (training) predictions\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        try:\n",
        "            y_train_proba = model.predict_proba(X_train)[:, 1]\n",
        "        except Exception:\n",
        "            try:\n",
        "                y_train_proba = model.decision_function(X_train)\n",
        "            except Exception:\n",
        "                y_train_proba = None\n",
        "        train_metrics = get_metrics(y_train, y_train_pred, y_train_proba)\n",
        "\n",
        "        # Out-of-sample (testing) predictions\n",
        "        y_test_pred = model.predict(X_test)\n",
        "        try:\n",
        "            y_test_proba = model.predict_proba(X_test)[:, 1]\n",
        "        except Exception:\n",
        "            try:\n",
        "                y_test_proba = model.decision_function(X_test)\n",
        "            except Exception:\n",
        "                y_test_proba = None\n",
        "        test_metrics = get_metrics(y_test, y_test_pred, y_test_proba)\n",
        "\n",
        "        train_metrics_list.append(train_metrics)\n",
        "        test_metrics_list.append(test_metrics)\n",
        "\n",
        "    # Average the metrics across folds.\n",
        "    avg_train_metrics = {k: np.mean([m[k] for m in train_metrics_list]) for k in train_metrics_list[0]}\n",
        "    avg_test_metrics = {k: np.mean([m[k] for m in test_metrics_list]) for k in test_metrics_list[0]}\n",
        "    return avg_train_metrics, avg_test_metrics\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# Main function: define all models and run evaluations for each balancing strategy.\n",
        "def run_all_models(X, y):\n",
        "    results = {}\n",
        "\n",
        "    # Define model instances.\n",
        "    models = {\n",
        "        'RandomForest': {\n",
        "            'base': RandomForestClassifier(random_state=42),\n",
        "            'class_weight': RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "        },\n",
        "        'LogisticRegression': {\n",
        "            'base': LogisticRegression(max_iter=2000, random_state=42),\n",
        "            'class_weight': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
        "        },\n",
        "        'SVM': {\n",
        "            'base': SVC(probability=True, random_state=42),\n",
        "            'class_weight': SVC(probability=True, class_weight='balanced', random_state=42)\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'base': xgb.XGBClassifier(eval_metric='logloss', random_state=42),\n",
        "            'class_weight': None  # Created dynamically below.\n",
        "        },\n",
        "        'NeuralNetwork': {\n",
        "            'base': MLPClassifier(max_iter=500, random_state=42)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    balancing_methods = ['none', 'smote', 'class_weight']\n",
        "\n",
        "    for model_name, model_dict in models.items():\n",
        "        results[model_name] = {}\n",
        "        for balancing in balancing_methods:\n",
        "            if model_name == 'XGBoost' and balancing == 'class_weight':\n",
        "                pos = np.sum(y == 1)\n",
        "                neg = np.sum(y == 0)\n",
        "                scale_pos_weight = neg / pos if pos > 0 else 1\n",
        "                model_instance = xgb.XGBClassifier(eval_metric='logloss',\n",
        "                                                   random_state=42,\n",
        "                                                   scale_pos_weight=scale_pos_weight)\n",
        "            elif balancing == 'class_weight' and model_name in ['RandomForest', 'LogisticRegression', 'SVM']:\n",
        "                model_instance = model_dict['class_weight']\n",
        "            else:\n",
        "                model_instance = model_dict['base']\n",
        "\n",
        "            model_instance = clone(model_instance)\n",
        "            train_metrics, test_metrics = evaluate_model(model_instance, X, y, balancing, cv_splits=5)\n",
        "            results[model_name][balancing] = {\n",
        "                'in_sample': train_metrics,\n",
        "                'out_sample': test_metrics\n",
        "            }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "P1b6X25w3dpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------------\n",
        "# Main execution:\n",
        "\n",
        "results = run_all_models(X, y)\n",
        "pprint.pprint(results)"
      ],
      "metadata": {
        "id": "wBp8WsNw33dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Transform the nested dictionary into a flat DataFrame.\n",
        "records = []\n",
        "for model, model_data in results.items():\n",
        "    for strategy, strat_data in model_data.items():\n",
        "        for sample in ['in_sample', 'out_sample']:\n",
        "            record = {\n",
        "                'Model': model,\n",
        "                'Balancing': strategy,\n",
        "                'Sample': sample\n",
        "            }\n",
        "            # Convert each metric to float and add to the record.\n",
        "            record.update({metric: float(val) for metric, val in strat_data[sample].items()})\n",
        "            records.append(record)\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Create a new column to combine Model and Balancing for easier plotting.\n",
        "df['Model_Balancing'] = df['Model'] + ' - ' + df['Balancing']\n",
        "\n",
        "# List of metrics to visualize.\n",
        "metrics = ['accuracy', 'auc', 'f1', 'precision', 'recall']\n",
        "\n",
        "# Set a Seaborn style.\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Loop over each metric and create a bar plot.\n",
        "for metric in metrics:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(data=df, x='Model_Balancing', y=metric, hue='Sample')\n",
        "    plt.title(f\"{metric.capitalize()} by Model and Balancing Strategy\")\n",
        "    plt.xlabel(\"Model and Balancing Strategy\")\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Loop over each metric and create a line plot\n",
        "for metric in metrics:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.lineplot(data=df, x='Model_Balancing', y=metric, hue='Sample', marker='o')\n",
        "    plt.title(f\"{metric.capitalize()} by Model and Balancing Strategy\")\n",
        "    plt.xlabel(\"Model and Balancing Strategy\")\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "mhep5PnIrCQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Further feature engineering\n",
        "# Split into training and test sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# ------------------------------\n",
        "# Tree-Based Feature Importance with RandomForest\n",
        "# ------------------------------\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 20 features\n",
        "print(\"Top 20 features by RandomForest:\")\n",
        "print(importance_df.head(20))\n",
        "print(\"\\nLast 20 features by RandomForest:\")\n",
        "print(importance_df.tail(20))\n",
        "\n",
        "\n",
        "# Plot the top 10 features.\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(importance_df['Feature'].head(10), importance_df['Importance'].head(10), color='skyblue')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Top 10 Feature Importances using RandomForest\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to show the most important at the top\n",
        "plt.show()\n",
        "\n",
        "# Plot the bottom 10 features.\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(importance_df['Feature'].tail(10), importance_df['Importance'].tail(10), color='skyblue')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Bottom 10 Feature Importances using RandomForest\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to show the most important at the top\n",
        "plt.show()\n",
        "\n",
        "# Plot top 10 features and the sum of bottom 50% features named \"Others\"\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(importance_df['Feature'].head(10), importance_df['Importance'].head(10), color='skyblue')\n",
        "others_importance = importance_df['Importance'].tail(50).sum()\n",
        "plt.barh('Others', others_importance, color='lightgray')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Top 10 and Bottom 50% Feature Importances using RandomForest\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "#Plot aggregated sum of feature importance by top 10, top 30, top 100 and the sum of the rest features\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_10 = importance_df['Importance'].head(10).sum()\n",
        "top_30 = importance_df['Importance'].head(30).sum()\n",
        "top_100 = importance_df['Importance'].head(100).sum()\n",
        "others_importance = importance_df['Importance'].sum() - top_100\n",
        "plt.barh('Top 10', top_10, color='darkred')\n",
        "plt.barh('Top 30', top_30, color='darkred')\n",
        "plt.barh('Top 100', top_100, color='darkred')\n",
        "plt.barh('Others', others_importance, color='lightgray')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Aggregated Feature Importances using RandomForest\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Recursive Feature Elimination (RFE) with Logistic Regression\n",
        "# ------------------------------\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Use Logistic Regression as the estimator; increase max_iter if needed.\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "# Here, we select the top 30 features. You can adjust n_features_to_select.\n",
        "rfe = RFE(estimator=lr, n_features_to_select=30)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "selected_features = X_train.columns[rfe.support_]\n",
        "ranking_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Ranking': rfe.ranking_\n",
        "}).sort_values(by='Ranking')\n",
        "\n",
        "print(\"Top 30 features selected by RFE (lower ranking is better):\")\n",
        "print(ranking_df.head(30))\n",
        "\n",
        "# Plot the rankings of the top 10 features.\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(ranking_df['Feature'].head(10), ranking_df['Ranking'].head(10), color='lightgreen')\n",
        "plt.xlabel(\"Feature Ranking (Lower is better)\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Top 20 Feature Rankings using RFE with Logistic Regression\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Statistical Feature Selection using SelectKBest (ANOVA F-test)\n",
        "# ------------------------------\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=\"all\")\n",
        "selector.fit(X_train, y_train)\n",
        "scores = selector.scores_\n",
        "pvalues = selector.pvalues_\n",
        "\n",
        "select_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Score': scores,\n",
        "    'p-value': pvalues\n",
        "}).sort_values(by='Score', ascending=False)\n",
        "\n",
        "print(\"Top 30 features by SelectKBest (ANOVA F-test):\")\n",
        "print(select_df.head(30))\n",
        "\n",
        "# Plot the scores for the top 20 features.\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(select_df['Feature'].head(20), select_df['Score'].head(20), color='salmon')\n",
        "plt.xlabel(\"F-score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Top 20 Features by ANOVA F-test using SelectKBest\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "INZ87bH94v0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 1. Tuning Random Forest\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.base import clone\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define a small grid for RandomForest hyperparameters.\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "balancing_methods = ['none', 'smote', 'class_weight']\n",
        "\n",
        "rf_results = {}\n",
        "\n",
        "for balancing in balancing_methods:\n",
        "    best_auc = -np.inf\n",
        "    best_params = None\n",
        "    best_in_sample = None\n",
        "    best_out_sample = None\n",
        "    for params in itertools.product(*param_grid_rf.values()):\n",
        "        param_dict = dict(zip(param_grid_rf.keys(), params))\n",
        "        # For class_weight balancing, add the parameter.\n",
        "        if balancing == 'class_weight':\n",
        "            param_dict['class_weight'] = 'balanced'\n",
        "        rf_model = RandomForestClassifier(random_state=42, **param_dict)\n",
        "\n",
        "        # Use your provided evaluate_model function (it applies balancing only on training set)\n",
        "        in_sample_metrics, out_sample_metrics = evaluate_model(clone(rf_model), X, y, balancing, cv_splits=5)\n",
        "        current_auc = out_sample_metrics['auc']\n",
        "        if current_auc > best_auc:\n",
        "            best_auc = current_auc\n",
        "            best_params = param_dict\n",
        "            best_in_sample = in_sample_metrics\n",
        "            best_out_sample = out_sample_metrics\n",
        "    rf_results[balancing] = {\n",
        "        'best_params': best_params,\n",
        "        'in_sample': best_in_sample,\n",
        "        'out_sample': best_out_sample\n",
        "    }\n",
        "\n",
        "print(\"RandomForest tuning results:\")\n",
        "print(rf_results)\n"
      ],
      "metadata": {
        "id": "wsl_McPb3lVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 2. Tuning XGB\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.base import clone\n",
        "import xgboost as xgb\n",
        "\n",
        "# Define a small grid for XGBoost hyperparameters.\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 6],\n",
        "    'learning_rate': [0.1, 0.01]\n",
        "}\n",
        "\n",
        "balancing_methods = ['none', 'smote', 'class_weight']\n",
        "xgb_results = {}\n",
        "\n",
        "for balancing in balancing_methods:\n",
        "    best_auc = -np.inf\n",
        "    best_params = None\n",
        "    best_in_sample = None\n",
        "    best_out_sample = None\n",
        "    for params in itertools.product(*param_grid_xgb.values()):\n",
        "        param_dict = dict(zip(param_grid_xgb.keys(), params))\n",
        "        # For XGBoost using class_weight, compute scale_pos_weight from y.\n",
        "        if balancing == 'class_weight':\n",
        "            pos = np.sum(y == 1)\n",
        "            neg = np.sum(y == 0)\n",
        "            param_dict['scale_pos_weight'] = neg / pos if pos > 0 else 1\n",
        "        xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42, **param_dict)\n",
        "        in_sample_metrics, out_sample_metrics = evaluate_model(clone(xgb_model), X, y, balancing, cv_splits=5)\n",
        "        current_auc = out_sample_metrics['auc']\n",
        "        if current_auc > best_auc:\n",
        "            best_auc = current_auc\n",
        "            best_params = param_dict\n",
        "            best_in_sample = in_sample_metrics\n",
        "            best_out_sample = out_sample_metrics\n",
        "    xgb_results[balancing] = {\n",
        "        'best_params': best_params,\n",
        "        'in_sample': best_in_sample,\n",
        "        'out_sample': best_out_sample\n",
        "    }\n",
        "\n",
        "print(\"XGBoost tuning results:\")\n",
        "print(xgb_results)\n"
      ],
      "metadata": {
        "id": "sBimHp0T4U9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3. Tuning Log\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.base import clone\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define a small grid for Logistic Regression hyperparameters.\n",
        "param_grid_lr = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'max_iter': [2000]  # Increased to help with convergence.\n",
        "}\n",
        "\n",
        "balancing_methods = ['none', 'smote', 'class_weight']\n",
        "lr_results = {}\n",
        "\n",
        "for balancing in balancing_methods:\n",
        "    best_auc = -np.inf\n",
        "    best_params = None\n",
        "    best_in_sample = None\n",
        "    best_out_sample = None\n",
        "    for params in itertools.product(*param_grid_lr.values()):\n",
        "        param_dict = dict(zip(param_grid_lr.keys(), params))\n",
        "        if balancing == 'class_weight':\n",
        "            param_dict['class_weight'] = 'balanced'\n",
        "        lr_model = LogisticRegression(random_state=42, **param_dict)\n",
        "        in_sample_metrics, out_sample_metrics = evaluate_model(clone(lr_model), X, y, balancing, cv_splits=5)\n",
        "        current_auc = out_sample_metrics['auc']\n",
        "        if current_auc > best_auc:\n",
        "            best_auc = current_auc\n",
        "            best_params = param_dict\n",
        "            best_in_sample = in_sample_metrics\n",
        "            best_out_sample = out_sample_metrics\n",
        "    lr_results[balancing] = {\n",
        "        'best_params': best_params,\n",
        "        'in_sample': best_in_sample,\n",
        "        'out_sample': best_out_sample\n",
        "    }\n",
        "\n",
        "print(\"Logistic Regression tuning results:\")\n",
        "print(lr_results)\n"
      ],
      "metadata": {
        "id": "RCwfez2C4eNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 4. Next-round tuning\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Improved grid for RandomForest to reduce overfitting:\n",
        "param_grid_rf_improved = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 8, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'max_features': [None, 'sqrt']\n",
        "}\n",
        "\n",
        "balancing_methods = ['none', 'smote', 'class_weight']\n",
        "rf_improved_results = {}\n",
        "\n",
        "for balancing in balancing_methods:\n",
        "    print(f\"\\nTuning RandomForest with balancing strategy: '{balancing}'\")\n",
        "    grid = list(itertools.product(*param_grid_rf_improved.values()))\n",
        "    total_fits = len(grid)\n",
        "    print(f\"Total parameter combinations to evaluate: {total_fits}\")\n",
        "\n",
        "    best_auc = -np.inf\n",
        "    best_params = None\n",
        "    best_in_sample = None\n",
        "    best_out_sample = None\n",
        "\n",
        "    for params in grid:\n",
        "        param_dict = dict(zip(param_grid_rf_improved.keys(), params))\n",
        "        if balancing == 'class_weight':\n",
        "            param_dict['class_weight'] = 'balanced'\n",
        "\n",
        "        rf_model = RandomForestClassifier(random_state=42, **param_dict)\n",
        "        in_sample_metrics, out_sample_metrics = evaluate_model(clone(rf_model), X, y, balancing, cv_splits=5)\n",
        "        current_auc = out_sample_metrics['auc']\n",
        "\n",
        "        if current_auc > best_auc:\n",
        "            best_auc = current_auc\n",
        "            best_params = param_dict\n",
        "            best_in_sample = in_sample_metrics\n",
        "            best_out_sample = out_sample_metrics\n",
        "\n",
        "    rf_improved_results[balancing] = {\n",
        "        'best_params': best_params,\n",
        "        'in_sample': best_in_sample,\n",
        "        'out_sample': best_out_sample\n",
        "    }\n",
        "\n",
        "    print(f\"Best results for RandomForest with balancing '{balancing}':\")\n",
        "    print(f\"  Best parameters: {best_params}\")\n",
        "    print(\"  In-sample metrics:\")\n",
        "    for metric, value in best_in_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "    print(\"  Out-of-sample metrics:\")\n",
        "    for metric, value in best_out_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Improved RandomForest Tuning Results:\")\n",
        "print(rf_improved_results)\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "import xgboost as xgb\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Improved grid for XGBoost with additional regularization parameters.\n",
        "param_grid_xgb_improved = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [2, 3, 4],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'reg_lambda': [0, 1],  # L2 regularization parameter.\n",
        "    'reg_alpha': [0, 0.1]   # L1 regularization parameter.\n",
        "}\n",
        "\n",
        "balancing_methods = ['none', 'smote', 'class_weight']\n",
        "xgb_improved_results = {}\n",
        "\n",
        "for balancing in balancing_methods:\n",
        "    print(f\"\\nTuning XGBoost with balancing strategy: '{balancing}'\")\n",
        "    grid = list(itertools.product(*param_grid_xgb_improved.values()))\n",
        "    total_fits = len(grid)\n",
        "    print(f\"Total parameter combinations to evaluate: {total_fits}\")\n",
        "\n",
        "    best_auc = -np.inf\n",
        "    best_params = None\n",
        "    best_in_sample = None\n",
        "    best_out_sample = None\n",
        "\n",
        "    for params in grid:\n",
        "        param_dict = dict(zip(param_grid_xgb_improved.keys(), params))\n",
        "        if balancing == 'class_weight':\n",
        "            pos = np.sum(y == 1)\n",
        "            neg = np.sum(y == 0)\n",
        "            param_dict['scale_pos_weight'] = neg / pos if pos > 0 else 1\n",
        "\n",
        "        xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42, **param_dict)\n",
        "        in_sample_metrics, out_sample_metrics = evaluate_model(clone(xgb_model), X, y, balancing, cv_splits=5)\n",
        "        current_auc = out_sample_metrics['auc']\n",
        "\n",
        "        if current_auc > best_auc:\n",
        "            best_auc = current_auc\n",
        "            best_params = param_dict\n",
        "            best_in_sample = in_sample_metrics\n",
        "            best_out_sample = out_sample_metrics\n",
        "\n",
        "    xgb_improved_results[balancing] = {\n",
        "        'best_params': best_params,\n",
        "        'in_sample': best_in_sample,\n",
        "        'out_sample': best_out_sample\n",
        "    }\n",
        "\n",
        "    print(f\"Best results for XGBoost with balancing '{balancing}':\")\n",
        "    print(f\"  Best parameters: {best_params}\")\n",
        "    print(\"  In-sample metrics:\")\n",
        "    for metric, value in best_in_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "    print(\"  Out-of-sample metrics:\")\n",
        "    for metric, value in best_out_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Improved XGBoost Tuning Results:\")\n",
        "print(xgb_improved_results)\n"
      ],
      "metadata": {
        "id": "k1Au65wN6t9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 5.0 tuning\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Reduced grid for RandomForest\n",
        "param_grid_rf_fast = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'max_depth': [3, 5],\n",
        "    'min_samples_split': [5, 10],\n",
        "    'max_features': [None, 'sqrt']\n",
        "}\n",
        "\n",
        "balancing_methods = ['none', 'smote', 'class_weight']\n",
        "rf_fast_results = {}\n",
        "\n",
        "for balancing in balancing_methods:\n",
        "    print(f\"\\nTuning RandomForest with balancing strategy: '{balancing}'\")\n",
        "    grid = list(itertools.product(*param_grid_rf_fast.values()))\n",
        "    total_fits = len(grid)\n",
        "    print(f\"Total parameter combinations to evaluate: {total_fits}\")\n",
        "\n",
        "    best_auc = -np.inf\n",
        "    best_params = None\n",
        "    best_in_sample = None\n",
        "    best_out_sample = None\n",
        "\n",
        "    for params in grid:\n",
        "        param_dict = dict(zip(param_grid_rf_fast.keys(), params))\n",
        "        if balancing == 'class_weight':\n",
        "            param_dict['class_weight'] = 'balanced'\n",
        "\n",
        "        rf_model = RandomForestClassifier(random_state=42, **param_dict)\n",
        "        in_sample_metrics, out_sample_metrics = evaluate_model(clone(rf_model), X, y, balancing, cv_splits=5)\n",
        "        current_auc = out_sample_metrics['auc']\n",
        "\n",
        "        if current_auc > best_auc:\n",
        "            best_auc = current_auc\n",
        "            best_params = param_dict\n",
        "            best_in_sample = in_sample_metrics\n",
        "            best_out_sample = out_sample_metrics\n",
        "\n",
        "    rf_fast_results[balancing] = {\n",
        "        'best_params': best_params,\n",
        "        'in_sample': best_in_sample,\n",
        "        'out_sample': best_out_sample\n",
        "    }\n",
        "\n",
        "    print(f\"\\nBest results for RandomForest with balancing '{balancing}':\")\n",
        "    print(f\"  Best parameters: {best_params}\")\n",
        "    print(\"  In-sample metrics:\")\n",
        "    for metric, value in best_in_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "    print(\"  Out-of-sample metrics:\")\n",
        "    for metric, value in best_out_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Faster RandomForest Tuning Results:\")\n",
        "print(rf_fast_results)\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "import xgboost as xgb\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Reduced grid for XGBoost (16 combinations, ignoring reg_alpha for speed)\n",
        "param_grid_xgb_fast = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [2, 3],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'reg_lambda': [0, 1]\n",
        "}\n",
        "\n",
        "balancing_methods = ['none', 'smote', 'class_weight']\n",
        "xgb_fast_results = {}\n",
        "\n",
        "for balancing in balancing_methods:\n",
        "    print(f\"\\nTuning XGBoost with balancing strategy: '{balancing}'\")\n",
        "    grid = list(itertools.product(*param_grid_xgb_fast.values()))\n",
        "    total_fits = len(grid)\n",
        "    print(f\"Total parameter combinations to evaluate: {total_fits}\")\n",
        "\n",
        "    best_auc = -np.inf\n",
        "    best_params = None\n",
        "    best_in_sample = None\n",
        "    best_out_sample = None\n",
        "\n",
        "    for params in grid:\n",
        "        param_dict = dict(zip(param_grid_xgb_fast.keys(), params))\n",
        "        if balancing == 'class_weight':\n",
        "            pos = np.sum(y == 1)\n",
        "            neg = np.sum(y == 0)\n",
        "            param_dict['scale_pos_weight'] = neg / pos if pos > 0 else 1\n",
        "\n",
        "        xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42, **param_dict)\n",
        "        in_sample_metrics, out_sample_metrics = evaluate_model(clone(xgb_model), X, y, balancing, cv_splits=5)\n",
        "        current_auc = out_sample_metrics['auc']\n",
        "\n",
        "        if current_auc > best_auc:\n",
        "            best_auc = current_auc\n",
        "            best_params = param_dict\n",
        "            best_in_sample = in_sample_metrics\n",
        "            best_out_sample = out_sample_metrics\n",
        "\n",
        "    xgb_fast_results[balancing] = {\n",
        "        'best_params': best_params,\n",
        "        'in_sample': best_in_sample,\n",
        "        'out_sample': best_out_sample\n",
        "    }\n",
        "\n",
        "    print(f\"\\nBest results for XGBoost with balancing '{balancing}':\")\n",
        "    print(f\"  Best parameters: {best_params}\")\n",
        "    print(\"  In-sample metrics:\")\n",
        "    for metric, value in best_in_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "    print(\"  Out-of-sample metrics:\")\n",
        "    for metric, value in best_out_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Faster XGBoost Tuning Results:\")\n",
        "print(xgb_fast_results)\n"
      ],
      "metadata": {
        "id": "50SwYdpF-F6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Custom evaluation function for Logistic Regression pipelines.\n",
        "def evaluate_model_lr(model, X, y, balancing, cv_splits=5, random_state=42):\n",
        "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
        "    train_metrics_list = []\n",
        "    test_metrics_list = []\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        try:\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "        except AttributeError:\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        sample_weights = None\n",
        "        if balancing == 'smote':\n",
        "            sm = SMOTE(random_state=random_state)\n",
        "            X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "        if balancing == 'class_weight':\n",
        "            # For pipelines, we do not pass sample_weight.\n",
        "            sample_weights = None\n",
        "\n",
        "        # For pipelines we avoid passing sample_weight.\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        try:\n",
        "            y_train_proba = model.predict_proba(X_train)[:, 1]\n",
        "        except Exception:\n",
        "            y_train_proba = None\n",
        "        train_metrics = {\n",
        "            'accuracy': accuracy_score(y_train, y_train_pred),\n",
        "            'precision': precision_score(y_train, y_train_pred, zero_division=0),\n",
        "            'recall': recall_score(y_train, y_train_pred, zero_division=0),\n",
        "            'f1': f1_score(y_train, y_train_pred, zero_division=0),\n",
        "            'auc': roc_auc_score(y_train, y_train_proba) if y_train_proba is not None else np.nan\n",
        "        }\n",
        "\n",
        "        y_test_pred = model.predict(X_test)\n",
        "        try:\n",
        "            y_test_proba = model.predict_proba(X_test)[:, 1]\n",
        "        except Exception:\n",
        "            y_test_proba = None\n",
        "        test_metrics = {\n",
        "            'accuracy': accuracy_score(y_test, y_test_pred),\n",
        "            'precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
        "            'recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
        "            'f1': f1_score(y_test, y_test_pred, zero_division=0),\n",
        "            'auc': roc_auc_score(y_test, y_test_proba) if y_test_proba is not None else np.nan\n",
        "        }\n",
        "\n",
        "        train_metrics_list.append(train_metrics)\n",
        "        test_metrics_list.append(test_metrics)\n",
        "\n",
        "    avg_train_metrics = {k: np.mean([m[k] for m in train_metrics_list]) for k in train_metrics_list[0]}\n",
        "    avg_test_metrics = {k: np.mean([m[k] for m in test_metrics_list]) for k in test_metrics_list[0]}\n",
        "    return avg_train_metrics, avg_test_metrics\n",
        "\n",
        "# Reduced grid for Logistic Regression (12 combinations)\n",
        "param_grid_lr_fast = {\n",
        "    'C': [0.001, 0.1, 1, 10],\n",
        "    'max_iter': [3000, 4000, 5000],\n",
        "    'solver': ['lbfgs', 'saga']\n",
        "}\n",
        "\n",
        "balancing_methods = ['none', 'smote', 'class_weight']\n",
        "lr_fast_results = {}\n",
        "\n",
        "for balancing in balancing_methods:\n",
        "    print(f\"\\nTuning Logistic Regression with balancing strategy: '{balancing}'\")\n",
        "    grid = list(itertools.product(*param_grid_lr_fast.values()))\n",
        "    total_fits = len(grid)\n",
        "    print(f\"Total parameter combinations to evaluate: {total_fits}\")\n",
        "\n",
        "    best_auc = -np.inf\n",
        "    best_params = None\n",
        "    best_in_sample = None\n",
        "    best_out_sample = None\n",
        "\n",
        "    for params in grid:\n",
        "        param_dict = dict(zip(param_grid_lr_fast.keys(), params))\n",
        "        if balancing == 'class_weight':\n",
        "            lr_model = LogisticRegression(class_weight='balanced', random_state=42, **param_dict)\n",
        "        else:\n",
        "            lr_model = LogisticRegression(random_state=42, **param_dict)\n",
        "\n",
        "        # Create pipeline with scaling.\n",
        "        pipe = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('lr', lr_model)\n",
        "        ])\n",
        "\n",
        "        in_sample_metrics, out_sample_metrics = evaluate_model_lr(clone(pipe), X, y, balancing, cv_splits=5)\n",
        "        current_auc = out_sample_metrics['auc']\n",
        "\n",
        "        if current_auc > best_auc:\n",
        "            best_auc = current_auc\n",
        "            best_params = param_dict\n",
        "            best_in_sample = in_sample_metrics\n",
        "            best_out_sample = out_sample_metrics\n",
        "\n",
        "    lr_fast_results[balancing] = {\n",
        "        'best_params': best_params,\n",
        "        'in_sample': best_in_sample,\n",
        "        'out_sample': best_out_sample\n",
        "    }\n",
        "\n",
        "    print(f\"\\nBest results for Logistic Regression with balancing '{balancing}':\")\n",
        "    print(f\"  Best parameters: {best_params}\")\n",
        "    print(\"  In-sample metrics:\")\n",
        "    for metric, value in best_in_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "    print(\"  Out-of-sample metrics:\")\n",
        "    for metric, value in best_out_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Faster Logistic Regression Tuning Results:\")\n",
        "print(lr_fast_results)"
      ],
      "metadata": {
        "id": "ScWErG_FCyRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Final grid for RandomForest\n",
        "param_grid_rf_final = {\n",
        "    'n_estimators': [50, 100,150],\n",
        "    'max_depth': [2, 3, 4],\n",
        "    'min_samples_split': [5, 10],\n",
        "    'max_features': [None, 'sqrt','log2']\n",
        "}\n",
        "\n",
        "balancing_methods = ['none', 'smote', 'class_weight']\n",
        "rf_final_results = {}\n",
        "\n",
        "for balancing in balancing_methods:\n",
        "    print(f\"\\nFinal Tuning: RandomForest with balancing strategy: '{balancing}'\")\n",
        "    grid = list(itertools.product(*param_grid_rf_final.values()))\n",
        "    total_fits = len(grid)\n",
        "    print(f\"Total parameter combinations to evaluate: {total_fits}\")\n",
        "\n",
        "    best_auc = -np.inf\n",
        "    best_params = None\n",
        "    best_in_sample = None\n",
        "    best_out_sample = None\n",
        "\n",
        "    for params in grid:\n",
        "        param_dict = dict(zip(param_grid_rf_final.keys(), params))\n",
        "        if balancing == 'class_weight':\n",
        "            param_dict['class_weight'] = 'balanced'\n",
        "\n",
        "        rf_model = RandomForestClassifier(random_state=42, **param_dict)\n",
        "        in_sample_metrics, out_sample_metrics = evaluate_model(clone(rf_model), X, y, balancing, cv_splits=5)\n",
        "        current_auc = out_sample_metrics['auc']\n",
        "\n",
        "        if current_auc > best_auc:\n",
        "            best_auc = current_auc\n",
        "            best_params = param_dict\n",
        "            best_in_sample = in_sample_metrics\n",
        "            best_out_sample = out_sample_metrics\n",
        "\n",
        "    rf_final_results[balancing] = {\n",
        "        'best_params': best_params,\n",
        "        'in_sample': best_in_sample,\n",
        "        'out_sample': best_out_sample\n",
        "    }\n",
        "\n",
        "    print(f\"\\nBest results for RandomForest with balancing '{balancing}':\")\n",
        "    print(f\"  Best parameters: {best_params}\")\n",
        "    print(\"  In-sample metrics:\")\n",
        "    for metric, value in best_in_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "    print(\"  Out-of-sample metrics:\")\n",
        "    for metric, value in best_out_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nFinal RandomForest Tuning Results:\")\n",
        "print(rf_final_results)\n"
      ],
      "metadata": {
        "id": "2v0mMXcbcmHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Narrow grid for Neural Network (MLPClassifier)\n",
        "param_grid_nn_final = {\n",
        "    'hidden_layer_sizes': [(50,), (100,)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'alpha': [0.0001, 0.001]\n",
        "}\n",
        "\n",
        "# For NN, we test only \"none\" and \"smote\" since class_weight is not supported.\n",
        "balancing_methods_nn = ['none', 'smote']\n",
        "nn_final_results = {}\n",
        "\n",
        "for balancing in balancing_methods_nn:\n",
        "    print(f\"\\nFinal Tuning: Neural Network (MLPClassifier) with balancing strategy: '{balancing}'\")\n",
        "    grid = list(itertools.product(*param_grid_nn_final.values()))\n",
        "    total_fits = len(grid)\n",
        "    print(f\"Total parameter combinations to evaluate: {total_fits}\")\n",
        "\n",
        "    best_auc = -np.inf\n",
        "    best_params = None\n",
        "    best_in_sample = None\n",
        "    best_out_sample = None\n",
        "\n",
        "    for params in grid:\n",
        "        param_dict = dict(zip(param_grid_nn_final.keys(), params))\n",
        "        nn_model = MLPClassifier(random_state=42, max_iter=500, **param_dict)\n",
        "\n",
        "        # Create pipeline with scaling.\n",
        "        pipe = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('nn', nn_model)\n",
        "        ])\n",
        "\n",
        "        in_sample_metrics, out_sample_metrics = evaluate_model(clone(pipe), X, y, balancing, cv_splits=5)\n",
        "        current_auc = out_sample_metrics['auc']\n",
        "\n",
        "        if current_auc > best_auc:\n",
        "            best_auc = current_auc\n",
        "            best_params = param_dict\n",
        "            best_in_sample = in_sample_metrics\n",
        "            best_out_sample = out_sample_metrics\n",
        "\n",
        "    nn_final_results[balancing] = {\n",
        "        'best_params': best_params,\n",
        "        'in_sample': best_in_sample,\n",
        "        'out_sample': best_out_sample\n",
        "    }\n",
        "\n",
        "    print(f\"\\nBest results for Neural Network with balancing '{balancing}':\")\n",
        "    print(f\"  Best parameters: {best_params}\")\n",
        "    print(\"  In-sample metrics:\")\n",
        "    for metric, value in best_in_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "    print(\"  Out-of-sample metrics:\")\n",
        "    for metric, value in best_out_sample.items():\n",
        "        print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Neural Network Tuning Results:\")\n",
        "print(nn_final_results)\n"
      ],
      "metadata": {
        "id": "kfjtlarcctMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Section 1: Further Tuning Experiments\n",
        "# -------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Helper function to compute metrics.\n",
        "def get_metrics(y_true, y_pred, y_proba):\n",
        "    metrics = {}\n",
        "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "    metrics['precision'] = precision_score(y_true, y_pred, zero_division=0)\n",
        "    metrics['recall'] = recall_score(y_true, y_pred, zero_division=0)\n",
        "    metrics['f1'] = f1_score(y_true, y_pred, zero_division=0)\n",
        "    if y_proba is not None:\n",
        "        metrics['auc'] = roc_auc_score(y_true, y_proba)\n",
        "    else:\n",
        "        metrics['auc'] = np.nan\n",
        "    return metrics\n",
        "\n",
        "# Evaluation function: using stratified k-fold CV.\n",
        "def evaluate_pipeline(estimator, X, y, cv_splits=5):\n",
        "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
        "    train_metrics_list = []\n",
        "    test_metrics_list = []\n",
        "\n",
        "    for train_idx, test_idx in skf.split(X, y):\n",
        "        try:\n",
        "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        except AttributeError:\n",
        "            X_train, X_test = X[train_idx], X[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        estimator.fit(X_train, y_train)\n",
        "        y_train_pred = estimator.predict(X_train)\n",
        "        try:\n",
        "            y_train_proba = estimator.predict_proba(X_train)[:, 1]\n",
        "        except Exception:\n",
        "            y_train_proba = None\n",
        "        train_metrics_list.append(get_metrics(y_train, y_train_pred, y_train_proba))\n",
        "\n",
        "        y_test_pred = estimator.predict(X_test)\n",
        "        try:\n",
        "            y_test_proba = estimator.predict_proba(X_test)[:, 1]\n",
        "        except Exception:\n",
        "            y_test_proba = None\n",
        "        test_metrics_list.append(get_metrics(y_test, y_test_pred, y_test_proba))\n",
        "\n",
        "    avg_train = {k: np.mean([m[k] for m in train_metrics_list]) for k in train_metrics_list[0]}\n",
        "    avg_test  = {k: np.mean([m[k] for m in test_metrics_list]) for k in test_metrics_list[0]}\n",
        "    return avg_train, avg_test\n",
        "\n",
        "# Convenience function to perform hyperparameter tuning.\n",
        "def tune_and_evaluate(pipeline, param_grid, X, y, cv_search=3, cv_eval=5):\n",
        "    grid = GridSearchCV(pipeline, param_grid, scoring='roc_auc', cv=cv_search, n_jobs=-1, error_score='raise')\n",
        "    grid.fit(X, y)\n",
        "    best_estimator = grid.best_estimator_\n",
        "    print(\"Best parameters:\", grid.best_params_)\n",
        "    in_sample, out_sample = evaluate_pipeline(best_estimator, X, y, cv_splits=cv_eval)\n",
        "    print(\"In-sample metrics:\", in_sample)\n",
        "    print(\"Out-of-sample metrics:\", out_sample)\n",
        "    return best_estimator\n",
        "\n",
        "# (Assume X and y are already defined in your environment.)\n",
        "\n",
        "# -----------------------------------------\n",
        "# RandomForest Further Tuning\n",
        "# -----------------------------------------\n",
        "\n",
        "# -- RandomForest with SMOTE --\n",
        "print(\"Further tuning: RandomForest with SMOTE:\")\n",
        "pipeline_rf_smote = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "param_grid_rf_smote = {\n",
        "    'classifier__n_estimators': [100, 150],\n",
        "    'classifier__max_depth': [4, 5, 6],\n",
        "    'classifier__min_samples_leaf': [1, 2],\n",
        "    'classifier__max_features': ['sqrt', 'log2']\n",
        "}\n",
        "best_rf_smote = tune_and_evaluate(pipeline_rf_smote, param_grid_rf_smote, X, y)\n",
        "\n",
        "# -- RandomForest with Class-Weight --\n",
        "print(\"\\nFurther tuning: RandomForest with Class-Weight:\")\n",
        "pipeline_rf_class = Pipeline([\n",
        "    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
        "])\n",
        "param_grid_rf_class = {\n",
        "    'classifier__n_estimators': [100, 150],\n",
        "    'classifier__max_depth': [4, 5, 6],\n",
        "    'classifier__min_samples_leaf': [1, 2],\n",
        "    'classifier__max_features': ['sqrt', 'log2']\n",
        "}\n",
        "best_rf_class = tune_and_evaluate(pipeline_rf_class, param_grid_rf_class, X, y)\n",
        "\n",
        "# -----------------------------------------\n",
        "# XGBoost Further Tuning\n",
        "# -----------------------------------------\n",
        "\n",
        "# -- XGBoost with SMOTE --\n",
        "print(\"\\nFurther tuning: XGBoost with SMOTE:\")\n",
        "pipeline_xgb_smote = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', xgb.XGBClassifier(eval_metric='logloss', random_state=42))\n",
        "])\n",
        "param_grid_xgb_smote = {\n",
        "    'classifier__n_estimators': [50, 75, 100],\n",
        "    'classifier__max_depth': [3, 4, 5],\n",
        "    'classifier__learning_rate': [0.01],\n",
        "    'classifier__reg_alpha': [0.05, 0.1, 0.2],\n",
        "    'classifier__reg_lambda': [1.5, 2, 2.5],\n",
        "    'classifier__subsample': [0.9, 1.0]\n",
        "}\n",
        "best_xgb_smote = tune_and_evaluate(pipeline_xgb_smote, param_grid_xgb_smote, X, y)\n",
        "\n",
        "# -- XGBoost with Class-Weight --\n",
        "print(\"\\nFurther tuning: XGBoost with Class-Weight:\")\n",
        "pos = np.sum(y == 1)\n",
        "neg = np.sum(y == 0)\n",
        "scale_weight = neg / pos if pos > 0 else 1\n",
        "pipeline_xgb_class = Pipeline([\n",
        "    ('classifier', xgb.XGBClassifier(eval_metric='logloss', random_state=42, scale_pos_weight=scale_weight))\n",
        "])\n",
        "param_grid_xgb_class = {\n",
        "    'classifier__n_estimators': [100, 125],\n",
        "    'classifier__max_depth': [3, 4],\n",
        "    'classifier__learning_rate': [0.01],\n",
        "    'classifier__reg_alpha': [0.005, 0.01, 0.02],\n",
        "    'classifier__reg_lambda': [1.5, 2],\n",
        "    'classifier__subsample': [0.8, 0.9]\n",
        "}\n",
        "best_xgb_class = tune_and_evaluate(pipeline_xgb_class, param_grid_xgb_class, X, y)\n",
        "\n",
        "# -----------------------------------------\n",
        "# NeuralNetwork Further Tuning\n",
        "# -----------------------------------------\n",
        "print(\"\\nFurther tuning: NeuralNetwork with SMOTE:\")\n",
        "pipeline_nn_smote = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', MLPClassifier(max_iter=500, random_state=42, early_stopping=True))\n",
        "])\n",
        "param_grid_nn_smote = {\n",
        "    'classifier__hidden_layer_sizes': [(50,), (100,), (150,)],\n",
        "    'classifier__alpha': [0.0005, 0.001, 0.005]\n",
        "}\n",
        "best_nn_smote = tune_and_evaluate(pipeline_nn_smote, param_grid_nn_smote, X, y)\n",
        "\n",
        "print(\"\\nFurther tuning: NeuralNetwork baseline (no balancing):\")\n",
        "pipeline_nn_baseline = Pipeline([\n",
        "    ('classifier', MLPClassifier(max_iter=500, random_state=42, early_stopping=True))\n",
        "])\n",
        "param_grid_nn_baseline = {\n",
        "    'classifier__hidden_layer_sizes': [(150,)],  # Zooming in on the best from previous run.\n",
        "    'classifier__alpha': [0.01]\n",
        "}\n",
        "best_nn_baseline = tune_and_evaluate(pipeline_nn_baseline, param_grid_nn_baseline, X, y)\n"
      ],
      "metadata": {
        "id": "oEAnlnz4UfU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. Store final results in a dictionary\n",
        "# -------------------------------------------------\n",
        "final_results = {\n",
        "    'RF': {'accuracy': 0.751752752064637, 'precision': 0.667384661328898,\n",
        "                       'recall': 0.5131008881958099, 'f1': 0.5797464995220165,\n",
        "                       'auc': 0.7845614024309545},\n",
        "    'XGB': {'accuracy': 0.7610659836335729, 'precision': 0.7017065805345479,\n",
        "                        'recall': 0.49389789138246665, 'f1': 0.5795234222357906,\n",
        "                        'auc': 0.7842233882483632},\n",
        "    'LR': {'accuracy': 0.7381631557635625, 'precision': 0.6346159477343123,\n",
        "                       'recall': 0.5090345108142925, 'f1': 0.5645008419194599,\n",
        "                       'auc': 0.7323059439650166}}\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. Convert results to a pandas DataFrame\n",
        "# -------------------------------------------------\n",
        "df = pd.DataFrame(final_results).T  # .T so that models become rows\n",
        "df.index.name = \"Model\"\n",
        "df.reset_index(inplace=True)  # Move model names into a column\n",
        "# df columns: Model, accuracy, precision, recall, f1, auc\n",
        "\n",
        "# Melt the data for easy plotting with seaborn\n",
        "df_melt = df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Value\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Define a custom darkredbased palette\n",
        "# -------------------------------------------------\n",
        "custom_palette = [\n",
        "    \"#840000\",  # darkest red\n",
        "    \"#b53232\",\n",
        "    \"#ce4b4b\",\n",
        "    \"#f08080\"   # lightest red\n",
        "]\n",
        "\n",
        "# Ensure we map each model to a unique color.\n",
        "# We'll just sort the model names alphabetically for consistent ordering\n",
        "model_names_sorted = sorted(df['Model'].unique())\n",
        "color_map = dict(zip(model_names_sorted, custom_palette))\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. Plot a Bar Chart for all metrics across models\n",
        "# -------------------------------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = [color_map[m] for m in model_names_sorted]\n",
        "\n",
        "sns.barplot(\n",
        "    data=df_melt,\n",
        "    x='Metric',\n",
        "    y='Value',\n",
        "    hue='Model',\n",
        "    palette=colors)\n",
        "plt.title('Performance Metrics Comparison (Bar Chart)', fontsize=14, color=\"#840101\")\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Metric Value', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.legend(title='Model', fontsize=10)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "#Show data labels\n",
        "for p in plt.gca().patches:\n",
        "    plt.gca().annotate(f\"{p.get_height():.3f}\", (p.get_x() + p.get_width() / 2.,\n",
        "                                                 p.get_height()),\n",
        "                       ha='center', va='center', fontsize=8,\n",
        "                       xytext=(0, 5), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. Plot a Line Chart for the same metrics\n",
        "# -------------------------------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(\n",
        "    data=df_melt,\n",
        "    x='Metric',\n",
        "    y='Value',\n",
        "    hue='Model',\n",
        "    style='Model',\n",
        "    markers=True,\n",
        "    dashes=False,\n",
        "    palette=[color_map[m] for m in df_melt['Model']],\n",
        "    linewidth=2,\n",
        "    markersize=10\n",
        ")\n",
        "plt.title('Performance Metrics Comparison (Line Chart)', fontsize=14, color=\"#840101\")\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Metric Value', fontsize=12)\n",
        "plt.xlabel('Metric', fontsize=12)\n",
        "plt.legend(title='Model', fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZL74cJhxAWYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, roc_curve)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import shap\n",
        "\n",
        "\n",
        "# Split into train and test (70/30 stratified split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Final hyperparameters based on tuning (using \"none\" balancing):\n",
        "lr_final = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LogisticRegression(C=1, max_iter=3000, solver='saga', random_state=42))\n",
        "])\n",
        "\n",
        "# Train the final model\n",
        "lr_final.fit(X_train, y_train)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Evaluate final models & save metrics\n",
        "# -------------------------------\n",
        "def get_metrics(y_true, y_pred, y_proba):\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
        "        'auc': roc_auc_score(y_true, y_proba) if y_proba is not None else np.nan\n",
        "    }\n",
        "\n",
        "final_metrics = {}\n",
        "models = {\n",
        "    'LogisticRegression': lr_final\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    # In-sample predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    try:\n",
        "        y_train_proba = model.predict_proba(X_train)[:, 1]\n",
        "    except Exception:\n",
        "        y_train_proba = None\n",
        "    in_metrics = get_metrics(y_train, y_train_pred, y_train_proba)\n",
        "\n",
        "    # Out-of-sample predictions\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    try:\n",
        "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
        "    except Exception:\n",
        "        y_test_proba = None\n",
        "    out_metrics = get_metrics(y_test, y_test_pred, y_test_proba)\n",
        "\n",
        "    final_metrics[name] = {'in_sample': in_metrics, 'out_sample': out_metrics}\n",
        "\n",
        "# Convert to DataFrame for visualization\n",
        "metrics_df = pd.DataFrame({\n",
        "    (model, sample): final_metrics[model][sample]\n",
        "    for model in final_metrics.keys()\n",
        "    for sample in ['in_sample', 'out_sample']\n",
        "}).T.reset_index()\n",
        "metrics_df.columns = ['Model', 'Sample', 'Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
        "print(\"Final Performance Metrics:\")\n",
        "print(metrics_df)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 4. SHAP Explanations\n",
        "# -------------------------------\n",
        "# Prepare feature names (if X is not a DataFrame, create default feature names)\n",
        "if hasattr(X, 'columns'):\n",
        "    feature_names = list(X.columns)\n",
        "else:\n",
        "    feature_names = [f\"f{i}\" for i in range(X.shape[1])]\n",
        "\n",
        "# Use a subset of training data for SHAP (for speed)\n",
        "X_shap = X_train[:100]\n",
        "\n",
        "\n",
        "#Logistic Regression SHAP\n",
        "\n",
        "X_shap_scaled = lr_final.named_steps['scaler'].transform(X_shap)\n",
        "explainer_lr = shap.LinearExplainer(lr_final.named_steps['lr'], X_shap_scaled, feature_perturbation=\"interventional\")\n",
        "shap_values_lr = explainer_lr.shap_values(X_shap_scaled)\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values_lr, X_shap_scaled, plot_type=\"bar\", color_bar_label=\"SHAP value\",\n",
        "                  show=False, plot_size=(8,6), feature_names=feature_names)\n",
        "plt.title('Logistic Regression SHAP Summary', fontsize=14, color='#006400')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i_35Xdn-KWtE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}